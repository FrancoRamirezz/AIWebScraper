# -*- coding: utf-8 -*-
"""AI webcrawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RYNgLwtDlOiAcflAo7Ju4tNukWJNZTOR
"""

# install the openai stuff
! pip install openai

! pip install tiktoken
! pip install cohere

!pip install -qU \
    openai==0.28.0 \
    langchain==0.0.301 \
    fastapi==0.103.1 \
    "uvicorn[standard]"==0.23.2

! pip install langchain

!pip install langchain --upgrade

!pip install tiktoken
! pip install playwright

import os
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.schema import HumanMessage
from langchain.document_loaders import WebBaseLoader

import os
os.environ["API key goes here"]

from langchain.memory import ConversationBufferWindowMemory
from langchain.agents import load_tools, AgentType, initialize_agent
from langchain.document_loaders import AsyncChromiumLoader
from langchain.document_transformers import BeautifulSoupTransformer
from langchain.document_loaders import AsyncHtmlLoader
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")

"""One version of scraping the entire website

**This is a simple version if we are looking for a specific aspect of the website**
"""

from langchain.document_loaders import AsyncHtmlLoader
urls = ["https://www.wsj.com"]
loader = AsyncHtmlLoader(urls) # this is used to make request
docs = loader.load()
# this is where we set the url to type
print(docs)



import pprint
from langchain.text_splitter import RecursiveCharacterTextSplitter

def scrape_with_playwright(urls, schema):

    loader = AsyncChromiumLoader(urls)
    docs = loader.load()
    bs_transformer = BeautifulSoupTransformer()
    docs_transformed = bs_transformer.transform_documents(docs,tags_to_extract=["span"])
    print("Extracting content with LLM")

    # Grab the first 1000 tokens of the site
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000,
                                                                    chunk_overlap=0)
    splits = splitter.split_documents(docs_transformed)

    # Process the first split
    extracted_content = extract(
        schema=schema, content=splits[0].page_content
    )
    pprint.pprint(extracted_content)
    return extracted_content

urls = ["https://www.wsj.com"]
extracted_content = scrape_with_playwright(urls, schema=schema)

"""**Now using beautiful soup and LLM to extract the HTML**"""

from langchain import schema
from langchain.document_transformers import BeautifulSoupTransformer
async def parsing_info(url:str, schema) -> str:
  loader = AsyncHtmlLoader(url)
  docs = loader.load()
  bot_Soup= BeautifulSoupTransformer()
  docs_transformed = schema.transform_documents(docs,tags_to_extract=["span"])
  schema = {
      "properties": {
        "product_name": {"type": "string"},
        "item_price": {"type": "intger"},
    },
    "required": ["product_name", "item_price"],


  }
# this will be used to run the async function for the
response = await parsing_info("https://www.espn.com/", schema = schema)
print(response)

from langchain.chains import create_extraction_chain

schema = {
    "properties": {
        "news_article_title": {"type": "string"},
        "news_article_summary": {"type": "string"},
    },
    "required": ["news_ar", "news_article_summary"],
}

def extract(content: str, schema: dict):
    return create_extraction_chain(schema=schema, llm=llm).run(content)

from langchain.document_loaders import AsyncChromiumLoader
from langchain.document_transformers import BeautifulSoupTransformer

# Load HTML
def scrape():
  loader = AsyncChromiumLoader(["https://www.wsj.com"])
  html = loader.load()
  bs_transformer = BeautifulSoupTransformer()
  docs_transformed = bs_transformer.transform_documents(docs,tags_to_extract=["span"])

import requests
from bs4 import BeautifulSoup
import langchain
import openai


# Get the list of all Y Combinator startups from the Hacker News YC startup list
response = requests.get("https://news.ycombinator.com/newstartups")
soup = BeautifulSoup(response.content, "html.parser")

# Extract the links to all the Y Combinator startup websites
links = []
for link in soup.find_all("a", class_="titlelink"):
    links.append(link["href"])

# Create a LangChain instance
lc = langchain.langChain()

# Create an OpenAI API key
openai_api_key = "YOUR_OPENAI_API_KEY"

# Define a function to scrape a Y Combinator startup website
def scrape_yc_startup(link):
    # Get the HTML of the Y Combinator startup website
    response = requests.get(link)
    soup = BeautifulSoup(response.content, "html.parser")

    # Extract the following information from the website:
    # * Startup name
    startup_name = soup.find("h1", class_="title").text

    # * Startup description
    startup_description = soup.find("div", class_="description").text

    # * Startup website URL
    startup_website_url = link

    # Use LangChain to generate a summary of the startup description
    startup_summary = lc.summarize(startup_description)

    # Use OpenAI to generate a list of keywords for the startup
    startup_keywords = openai.Engine("davinci").generate_keywords(startup_description)

    # Return the scraped data
    return {
        "startup_name": startup_name,
        "startup_description": startup_description,
        "startup_website_url": startup_website_url,
        "startup_summary": startup_summary,
        "startup_keywords": startup_keywords,
    }

# Scrape all the Y Combinator startup websites
scraped_data = []
for link in links:
    scraped_data.append(scrape_yc_startup(link))

# Save the scraped data to a file
with open("yc_startups.json", "w") as f:
    f.write(json.dumps(scraped_data, indent=4))













